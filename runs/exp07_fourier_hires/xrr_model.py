import numpy as np
import torch
import torch.nn as nn


class FourierFeatureMapping(nn.Module):
    """
    Random Fourier Features:
    좌표(q)를 고차원 주파수 공간으로 매핑하여 고주파(High-Frequency) 패턴 학습을 돕습니다.
    """
    def __init__(self, input_dim=1, mapping_size=64, scale=10.0):
        super().__init__()
        self.mapping_size = mapping_size
        self.register_buffer('B', torch.randn(input_dim, mapping_size) * scale)

    def forward(self, x):
        if x.dim() == 2:
            x = x.unsqueeze(1)
        x_proj = x.transpose(1, 2) @ self.B
        x_proj = x_proj.transpose(1, 2) * 2 * np.pi
        return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=1)

class XRRPhysicsModel(nn.Module):
    def __init__(self, q_len: int, input_channels: int = 2, output_dim: int = 3,
                n_channels: int = 64, depth: int = 5, mlp_hidden: int = 256,
                dropout: float = 0.1, use_fourier: bool = True, fourier_scale: float = 10.0):
        super().__init__()
        self.use_fourier = use_fourier

        # Checkpoint 호환성을 위한 Config 저장
        self.config = {
            'q_len': q_len, 'input_channels': input_channels, 'output_dim': output_dim,
            'n_channels': n_channels, 'depth': depth, 'mlp_hidden': mlp_hidden,
            'dropout': dropout, 'use_fourier': use_fourier, 'fourier_scale': fourier_scale
        }

        self.fourier = FourierFeatureMapping(input_dim=1, mapping_size=32, scale=fourier_scale)
        enc_in_channels = input_channels + (32 * 2) if use_fourier else input_channels
        self.register_buffer('q_grid', torch.linspace(0, 1, q_len).view(1, 1, -1))

        layers = []
        curr_dim = enc_in_channels
        for i in range(depth):
            out_dim = n_channels * (2 ** min(i, 3))
            layers.append(nn.Sequential(
                nn.Conv1d(curr_dim, out_dim, kernel_size=7, padding=3, bias=False),
                nn.BatchNorm1d(out_dim),
                nn.LeakyReLU(0.2, inplace=True),
                nn.Dropout1d(dropout),
                nn.MaxPool1d(kernel_size=2)
            ))
            curr_dim = out_dim

        self.encoder = nn.Sequential(*layers)
        self.global_pool = nn.AdaptiveAvgPool1d(1)
        self.regressor = nn.Sequential(
            nn.Linear(curr_dim, mlp_hidden),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Dropout(dropout),
            nn.Linear(mlp_hidden, mlp_hidden // 2),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(mlp_hidden // 2, output_dim)
        )
        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, (nn.Conv1d, nn.Linear)):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')

    def forward(self, x):
        bs = x.shape[0]
        if self.use_fourier:
            q_batch = self.q_grid.expand(bs, -1, -1)
            q_fourier = self.fourier(q_batch)
            x = torch.cat([x, q_fourier], dim=1)
        feat = self.encoder(x)
        feat = self.global_pool(feat).squeeze(-1)
        return self.regressor(feat)
